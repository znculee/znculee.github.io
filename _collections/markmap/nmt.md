# NMT

## Understanding Model Architecture

### Understanding encoder/decoder

#### Q. [Does encoder's representation entail liguistic knowledge?]()
#### Q. [Can encoder learn word sense disambiguation?]()
#### Q. [Does decoder's representation entail liguistic knowledge?]()
#### Q. [Which component of NMT is critical?]()

### Understanding Attention

#### Cross-attention

##### Q. [Does attention leran alignments?]()
##### Q. [Do attention weights reflect NMT's reasoning?]()

#### Self-attention

##### Q. [Is self-attention network better than RNN?]()
##### Q. [Is multi-head better than single-head?]()

## Understanding Training

### Training data

### Training loss

### Training tricks

#### Q. [How does LN help?]()
#### Q. [Which optimizer to use?]()
#### Q. [How does label smoothing help?]()

## Understanding Inference

### Prediction explanation

#### Q. [How to attribute NMT model's prediciton?]()
#### Q. [How to properly evaluate prediction attribution of NMT model?]()

### Decoding explanation

#### Q. [Do larger beams lead to better results?]()
#### Q. [Is beam search decoding not good enough?]()

## Understanding Model Behavior

### Static analysis

#### Q. [Is NMT model's prediction linguistically natural?]()
#### Q. [Can NMT mdoel generate long-tailed translation?]()
#### Q. [Can NMT model generate calibrated predictions?]()

### Controlled analysis

#### Q. [Can NMT model handle inputs with difference types of linguistic phenomenon?]()
#### Q. [Can NMT model handle inputs compositionally?]()

### Dynamic analysis

#### Q. [Is NMT model robust to inputs?]()
#### Q. [How does training data affect NMT model's prediction?]()
#### Q. [When or why does NMT model hallucinate?]()
